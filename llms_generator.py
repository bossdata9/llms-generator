# -*- coding: utf-8 -*-
"""LLMS.TXT Generator

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KCWIg035AuIZyowB_geDF7PxhcazaOsL

# Installatie van libraries
"""

"""# Configuratie + inladen van bestand"""

import os
import streamlit as st


### CONFIGURATION ###
input = "https://www.bossdata.be/"
os.environ["OPENAI_API_KEY"] = st.secrets["openai_api_key"]
size = 80
output = "/content/drive/Shared drives/Corporate/Operations/04. TECHNOLOGY/Notebooks/AI - LLMS.TXT/output/llms.txt"

"""# Script

## Fase 1: bestand uitlezen en omzetten in lijst
"""

import requests  # Allows us to make requests to websites
import xml.etree.ElementTree as ET  # Helps us work with XML data

def extract_urls_from_sitemap(sitemap_url):
    """
    Gets URLs from an XML sitemap. If the sitemap contains links to
    other sitemaps, it will also check those.

    Args:
        sitemap_url (str): The link to the sitemap.

    Returns:
        list: A list of URLs found in the sitemap(s).
    """
    response = requests.get(sitemap_url)  # Download the sitemap from the web

    if response.status_code != 200:  # If the request fails, show an error message
        print(f"Failed to fetch the sitemap: {sitemap_url}. Status code: {response.status_code}")
        return []  # Return an empty list if there's an error

    root = ET.fromstring(response.content)  # Convert the XML data into a format we can use
    namespace = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}  # Define the XML format

    urls = []  # A list to store the links we find

    # Find out if this sitemap links to other sitemaps
    sitemap_tags = root.findall('.//ns:sitemap/ns:loc', namespace)

    if sitemap_tags:
        # If there are other sitemaps, go through each one
        for sitemap in sitemap_tags:
            urls.extend(extract_urls_from_sitemap(sitemap.text))  # Get URLs from each sitemap
    else:
        # If it's a regular sitemap, find and save all URLs inside it
        urls = [loc.text for loc in root.findall('.//ns:loc', namespace)]

    return urls  # Return the list of found URLs

# Example: Get URLs from a sitemap and save them
sitemap_url = input + "page-sitemap.xml"
urls = extract_urls_from_sitemap(sitemap_url)

selected_urls = urls[:size]
print("selected urls:")
print(selected_urls)

"""##Fase 2: Enkel relevante URLS selecteren"""

import os
import json
import requests
from openai import OpenAI

# --- API key ---
client = OpenAI()

# --- Excluded segments ---
EXCLUDE_SEGMENTS = ["index", "home", "homepage", "privacy", "terms", "legal", "sitemap",
    "sitemap.xml", "robots.txt", "author", "authors", "admin", "login",
    "user-data", "settings", "internal-docs", "pricing", "sales-materials", "confidential", "beta", "staging", "dev", "404",
    "search", "thank-you", "cart", "tag", "category", "archive"
]

# --- Decide if page should be kept ---
def is_relevant_page(url: str, model="gpt-4.1-mini"):
    exclude_str = ", ".join(EXCLUDE_SEGMENTS)
    prompt = f"""
You are given a URL.
Assume the page content based on its path and structure.

A page is RELEVANT (content) if it is useful to end users
(e.g. articles, product/service pages, blog posts, guides).

A page is NOT RELEVANT (other) if it looks like a utility or irrelevant page
(e.g. privacy policy, terms, login, settings, cart, thank-you pages,
sitemaps, 404s, or if the URL contains any of these segments:
{exclude_str}).

Return only the word KEEP if the page should be included in the final list.
Return DROP otherwise. When in doubt, just use KEEP.

URL: {url}
"""
    resp = client.responses.create(
        model=model,
        input=[{"role": "user", "content": prompt}],
        temperature=0,
    )
    txt = resp.output_text.strip().upper()
    return txt == "KEEP"

# --- Main loop ---
relevant_urls = []
print(f"ℹ️ Starting URL filtering process for {len(selected_urls)} URLs...")
print("-" * 30)

for u in selected_urls:
    print(f"▶️ Processing: {u}")

    # Pre-filter obvious junk first
    if any(seg in u.lower() for seg in EXCLUDE_SEGMENTS):
        print(f"🗑️ Dropped by pre-filter: {u}")
        continue

    if is_relevant_page(u):
        print(f"✅ Kept: {u}")
        relevant_urls.append(u)
    else:
        print(f"❌ Dropped by relevance check: {u}")

print("-" * 30)
print(f"✅ Filtering complete! Found {len(relevant_urls)} relevant URLs.")
print("✅ Final relevant URLs:")
print(json.dumps(relevant_urls, indent=2))

"""##Fase3: Relevante URLS samenvatten"""

import os
import json
import time
import random
import requests
import trafilatura
from openai import OpenAI
import re

# --- API key ---
client = OpenAI()

# --- Simple fetch text helper ---
SESSION = requests.Session()
SESSION.headers.update({"User-Agent": "Mozilla/5.0"})

def fetch_text(url: str) -> str:
    try:
        r = SESSION.get(url, timeout=20)
        r.raise_for_status()
        text = trafilatura.extract(r.text)
        return text or ""
    except Exception:
        return ""

def describe_page(url: str, page_text: str, meta_title="", h1="", model="gpt-4.1-mini"):
    base_prompt = f"""You will summarize the page for: {url}

Rules:
- Focus on the page’s **primary content** (topic, offering, purpose).
- **Ignore** cookie banners, consent notices, privacy/terms/legal boilerplate, navigation, footers, and popups.
- Prefer signals from the HTML <title>, H1/H2 headings, and body copy that describe the main topic.

Constraints:
- Title: exactly 3–4 words, English.
- Description: exactly 9–10 words, plain, neutral English.
Return JSON with keys: title, description.

Page signals (if present):
- HTML title: {meta_title}
- H1: {h1}

--- PAGE CONTENT START ---
{page_text[:8000]}
--- PAGE CONTENT END ---
"""
    resp = client.responses.create(
        model=model,
        input=[{"role": "user", "content": base_prompt}],
        temperature=0.3,
    )
    txt = resp.output_text.strip()

    # Remove markdown code fences if present
    if txt.startswith("```"):
        txt = re.sub(r"^```[a-zA-Z]*\n?", "", txt)  # remove opening fence
        txt = re.sub(r"\n?```$", "", txt)           # remove closing fence
        txt = txt.strip()

    try:
        data = json.loads(txt)
        return {
            "url": url,
            "title": data.get("title", "").strip(),
            "description": data.get("description", "").strip()
        }
    except Exception:
        # fallback: just return the raw text in description
        return {"url": url, "title": "", "description": txt}

# --- Run for all URLs ---
results = []
for i, u in enumerate(relevant_urls, start=1):
    print(f"[{i}/{len(relevant_urls)}] {u}")
    page_text = fetch_text(u)
    item = describe_page(u, page_text)
    results.append(item)
    time.sleep(0.5 + random.random()*0.5)  # polite delay

# results is now a list of dicts
print("\n✅ Finished. Results:")
print(json.dumps(results, indent=2))

"""##Fase 4: clusteren en opmaken LLMS.TXT"""

import re
import json
import math
from urllib.parse import urlparse
from collections import defaultdict, Counter
from openai import OpenAI  # only needed if USE_EMBEDDINGS=True
from urllib.parse import urlparse
# --------------------
# CONFIG
# --------------------
USE_EMBEDDINGS = True                 # set False to skip semantic merging
EMBED_MODEL = "text-embedding-3-small"
SIM_THRESHOLD = 0.82                  # centroid cosine similarity to merge clusters


client = OpenAI()  # requires OPENAI_API_KEY when USE_EMBEDDINGS=True

# --------------------
# UTILS
# --------------------
def generate_site_description(results, model="gpt-4.1-mini"):
    """
    Use GPT to generate a general description of the website
    based on all titles + descriptions in results.
    """
    if not results:
        return "General Website", "No description available."

    # take domain from first URL
    first_url = results[0]["url"]
    domain = urlparse(first_url).netloc

    # collect page-level titles and descriptions
    sample_text = "\n".join(
        f"- {r.get('title','')} — {r.get('description','')}"
        for r in results[:30]  # limit to avoid super long prompts
    )

    prompt = f"""
You are given a list of page titles and descriptions from the website {domain}.

Write a concise general description (2–3 sentences, neutral English) that summarizes
the main purpose of the site as a whole.

Return only the description text, no extra formatting.

Pages:
{sample_text}
"""

    try:
        resp = client.responses.create(
            model=model,
            input=[{"role": "user", "content": prompt}],
            temperature=0,
        )
        site_description = resp.output_text.strip()
        return domain, site_description
    except Exception:
        return domain, "No description available."

def url_first_segment(u: str) -> str:
    p = urlparse(u)
    segs = [s for s in p.path.split("/") if s]
    return segs[0].lower() if segs else ""

def normalize_space(s: str) -> str:
    return re.sub(r"\s+", " ", s or "").strip()

def dot(a, b): return sum(x*y for x, y in zip(a, b))
def norm(a): return math.sqrt(dot(a, a)) or 1e-12
def cosine(a, b): return dot(a, b) / (norm(a) * norm(b))

def cluster_label_from_items(items, fallback="Content", model="gpt-4.1-mini"):
    """
    Ask GPT to propose a short English cluster label (2–4 words)
    based only on the titles of items in the cluster.
    """
    titles = [it.get("title", "").strip() for it in items if it.get("title")]
    if not titles:
        return fallback

    titles_text = "\n".join(f"- {t}" for t in titles if t)

    prompt = f"""
Given the following page titles, suggest a concise English cluster label (2–4 words)
that best represents them as a group.

Return only the label, with no quotes or extra text.

Titles:
{titles_text}
"""

    try:
        resp = client.responses.create(
            model=model,
            input=[{"role": "user", "content": prompt}],
            temperature=0,
        )
        label = resp.output_text.strip()
        return label if label else fallback
    except Exception:
        return fallback

# --------------------
# CLUSTERING
# --------------------
def seed_clusters(results):
    """
    Group by first path segment; root pages go to 'root'.
    results: list of dicts with keys url, title, description
    """
    buckets = defaultdict(list)
    for item in results:
        seg = url_first_segment(item.get("url","")) or "root"
        buckets[seg].append(item)
    # deterministically ordered by segment key
    return [items for _, items in sorted(buckets.items(), key=lambda kv: kv[0])]

def embed_texts(texts):
    resp = client.embeddings.create(model=EMBED_MODEL, input=texts)
    return [d.embedding for d in resp.data]

def represent_item(item):
    return normalize_space(f'{item.get("title","")} — {item.get("description","")}')

def cluster_centroid(vecs):
    if not vecs: return []
    dim = len(vecs[0])
    s = [0.0]*dim
    for v in vecs:
        for i in range(dim):
            s[i] += v[i]
    return [x/len(vecs) for x in s]

def semantically_merge(clusters):
    """
    Merge rule-based clusters if their centroids are similar (>= SIM_THRESHOLD).
    """
    # Build item-level embeddings
    texts = []
    idx_map = []  # (cluster_idx, item_idx)
    for ci, items in enumerate(clusters):
        for ii, it in enumerate(items):
            texts.append(represent_item(it))
            idx_map.append((ci, ii))
    if not texts:
        return clusters

    vecs = embed_texts(texts)

    # Aggregate by cluster -> centroid
    per_cluster_vecs = defaultdict(list)
    for (ci, _), v in zip(idx_map, vecs):
        per_cluster_vecs[ci].append(v)
    centroids = [cluster_centroid(per_cluster_vecs[i]) for i in range(len(clusters))]

    # Greedy merge
    merged = []
    used = set()
    for i in range(len(clusters)):
        if i in used:
            continue
        base_items = list(clusters[i])
        base_centroid = centroids[i]
        used.add(i)

        for j in range(i+1, len(clusters)):
            if j in used or not base_centroid or not centroids[j]:
                continue
            if cosine(base_centroid, centroids[j]) >= SIM_THRESHOLD:
                base_items.extend(clusters[j])
                used.add(j)
                # recompute centroid approximately by averaging previous centroids
                base_centroid = cluster_centroid([base_centroid, centroids[j]])

        merged.append(base_items)
    return merged

# --------------------
# WRITE llms.txt
# --------------------
def write_llms_txt(clusters, results, path=output):
    lines = []
    lines.append("# llms.txt")

    # --- Add global title + description ---
    domain, site_description = generate_site_description(results)
    lines.append(f"# {domain.title()}")
    lines.append("")
    lines.append(f"> {site_description}")
    lines.append("")

    # --- Clustered pages ---
    for cluster_items in clusters:
        label = cluster_label_from_items(cluster_items, fallback="Content")
        lines.append(f"## Cluster: {label}")
        for it in cluster_items:
            lines.append(f"- URL: {it.get('url','')}")
            lines.append(f"  Title: {normalize_space(it.get('title',''))}")
            lines.append(f"  Description: {normalize_space(it.get('description',''))}")
        lines.append("")

    with open(path, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))
    return path

# --------------------
# MAIN ENTRY (your case)
# --------------------
def build_llms_txt_from_results(results):
    """
    Input: results = [
        {"url": "...", "title": "...", "description": "..."},
        ...
    ]
    """
    # 1) Seed clusters (path-based)
    clusters = seed_clusters(results)

    # 2) Optional semantic merge (title+description)
    if USE_EMBEDDINGS and sum(len(c) for c in clusters) > 1:
        clusters = semantically_merge(clusters)

    # 3) Sort items within clusters by URL
    for c in clusters:
        c.sort(key=lambda it: it.get("url",""))

    # 4) Write llms.txt
    out = write_llms_txt(clusters, results, path=output)
    print(f"✅ Wrote {out}")

# --------------------
# Example:
# --------------------
build_llms_txt_from_results(results)

