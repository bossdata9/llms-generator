# -*- coding: utf-8 -*-
"""LLMS.TXT Generator

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KCWIg035AuIZyowB_geDF7PxhcazaOsL
"""

# ======================================================================
# All imports are now at the top of the file
# ======================================================================
import streamlit as st
import os
import requests
import xml.etree.ElementTree as ET
import json
from openai import OpenAI
import time
import random
import trafilatura
import re
from urllib.parse import urlparse
from collections import defaultdict, Counter
import math

# ======================================================================
# All functions are now defined here
# ======================================================================
def extract_urls_from_sitemap(sitemap_url):
    """
    Gets URLs from an XML sitemap. If the sitemap contains links to
    other sitemaps, it will also check those.
    """
    response = requests.get(sitemap_url)
    if response.status_code != 200:
        print(f"Failed to fetch the sitemap: {sitemap_url}. Status code: {response.status_code}")
        return []
    root = ET.fromstring(response.content)
    namespace = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}
    urls = []
    sitemap_tags = root.findall('.//ns:sitemap/ns:loc', namespace)
    if sitemap_tags:
        for sitemap in sitemap_tags:
            urls.extend(extract_urls_from_sitemap(sitemap.text))
    else:
        urls = [loc.text for loc in root.findall('.//ns:loc', namespace)]
    return urls

def is_relevant_page(url: str, client, EXCLUDE_SEGMENTS, model="gpt-4.1-mini"):
    exclude_str = ", ".join(EXCLUDE_SEGMENTS)
    prompt = f"""
You are given a URL.
Assume the page content based on its path and structure.

A page is RELEVANT (content) if it is useful to end users
(e.g. articles, product/service pages, blog posts, guides).

A page is NOT RELEVANT (other) if it looks like a utility or irrelevant page
(e.g. privacy policy, terms, login, settings, cart, thank-you pages,
sitemaps, 404s, or if the URL contains any of these segments:
{exclude_str}).

Return only the word KEEP if the page should be included in the final list.
Return DROP otherwise. When in doubt, just use KEEP.

URL: {url}
"""
    resp = client.responses.create(
        model=model,
        input=[{"role": "user", "content": prompt}],
        temperature=0,
    )
    txt = resp.output_text.strip().upper()
    return txt == "KEEP"

SESSION = requests.Session()
SESSION.headers.update({"User-Agent": "Mozilla/5.0"})

def fetch_text(url: str) -> str:
    try:
        r = SESSION.get(url, timeout=20)
        r.raise_for_status()
        text = trafilatura.extract(r.text)
        return text or ""
    except Exception:
        return ""

def describe_page(url: str, page_text: str, client, meta_title="", h1="", model="gpt-4.1-mini"):
    base_prompt = f"""You will summarize the page for: {url}

Rules:
- Focus on the page’s **primary content** (topic, offering, purpose).
- **Ignore** cookie banners, consent notices, privacy/terms/legal boilerplate, navigation, footers, and popups.
- Prefer signals from the HTML <title>, H1/H2 headings, and body copy that describe the main topic.

Constraints:
- Title: exactly 3–4 words, English.
- Description: exactly 9–10 words, plain, neutral English.
Return JSON with keys: title, description.

Page signals (if present):
- HTML title: {meta_title}
- H1: {h1}

--- PAGE CONTENT START ---
{page_text[:8000]}
--- PAGE CONTENT END ---
"""
    resp = client.responses.create(
        model=model,
        input=[{"role": "user", "content": base_prompt}],
        temperature=0.3,
    )
    txt = resp.output_text.strip()
    if txt.startswith("```"):
        txt = re.sub(r"^```[a-zA-Z]*\n?", "", txt)
        txt = re.sub(r"\n?```$", "", txt)
        txt = txt.strip()
    try:
        data = json.loads(txt)
        return {
            "url": url,
            "title": data.get("title", "").strip(),
            "description": data.get("description", "").strip()
        }
    except Exception:
        return {"url": url, "title": "", "description": txt}

def generate_site_description(results, client, model="gpt-4.1-mini"):
    if not results: return "General Website", "No description available."
    first_url = results[0]["url"]
    domain = urlparse(first_url).netloc.lower()
    sample_text = "\n".join(
        f"- {r.get('title','') or ''} — {r.get('description','') or ''}"
        for r in results[:30]
    )
    prompt = f"""
You are given a list of page titles and descriptions from the website {domain}.

Write a concise general description (2–3 sentences, neutral English) that summarizes
the main purpose of the site as a whole.

Return only the description text, no extra formatting.

Pages:
{sample_text}
"""
    try:
        resp = client.responses.create(
            model=model,
            input=[{"role": "user", "content": prompt}],
            temperature=0,
        )
        site_description = resp.output_text.strip()
        return domain, site_description
    except Exception: return domain, "No description available."

def url_first_segment(u: str) -> str:
    p = urlparse(u)
    segs = [s for s in p.path.split("/") if s]
    return segs[0].lower() if segs else ""

def normalize_space(s: str) -> str:
    return re.sub(r"\s+", " ", s or "").strip()

def dot(a, b): return sum(x*y for x, y in zip(a, b))
def norm(a): return math.sqrt(dot(a, a)) or 1e-12
def cosine(a, b): return dot(a, b) / (norm(a) * norm(b))

def cluster_label_from_items(items, client, fallback="Content", model="gpt-4.1-mini"):
    titles = [it.get("title", "").strip() for it in items if it.get("title")]
    if not titles: return fallback
    titles_text = "\n".join(f"- {t}" for t in titles if t)
    prompt = f"""
Given the following page titles, suggest a concise English cluster label (2–4 words)
that best represents them as a group.

Return only the label, with no quotes or extra text.

Titles:
{titles_text}
"""
    try:
        resp = client.responses.create(
            model=model,
            input=[{"role": "user", "content": prompt}],
            temperature=0,
        )
        label = resp.output_text.strip()
        return label if label else fallback
    except Exception: return fallback

def seed_clusters(results):
    buckets = defaultdict(list)
    for item in results:
        seg = url_first_segment(item.get("url","")) or "root"
        buckets[seg].append(item)
    return [items for _, items in sorted(buckets.items(), key=lambda kv: kv[0])]

def embed_texts(texts, client, EMBED_MODEL):
    resp = client.embeddings.create(model=EMBED_MODEL, input=texts)
    return [d.embedding for d in resp.data]

def represent_item(item):
    return normalize_space(f'{item.get("title","")} — {item.get("description","")}')

def cluster_centroid(vecs):
    if not vecs: return []
    dim = len(vecs[0])
    s = [0.0]*dim
    for v in vecs:
        for i in range(dim): s[i] += v[i]
    return [x/len(vecs) for x in s]

def semantically_merge(clusters, client, USE_EMBEDDINGS, EMBED_MODEL, SIM_THRESHOLD):
    texts = []
    idx_map = []
    for ci, items in enumerate(clusters):
        for ii, it in enumerate(items):
            texts.append(represent_item(it))
            idx_map.append((ci, ii))
    if not texts: return clusters
    vecs = embed_texts(texts, client, EMBED_MODEL)
    per_cluster_vecs = defaultdict(list)
    for (ci, _), v in zip(idx_map, vecs): per_cluster_vecs[ci].append(v)
    centroids = [cluster_centroid(per_cluster_vecs[i]) for i in range(len(clusters))]
    merged = []
    used = set()
    for i in range(len(clusters)):
        if i in used: continue
        base_items = list(clusters[i])
        base_centroid = centroids[i]
        used.add(i)
        for j in range(i+1, len(clusters)):
            if j in used or not base_centroid or not centroids[j]: continue
            if cosine(base_centroid, centroids[j]) >= SIM_THRESHOLD:
                base_items.extend(clusters[j])
                used.add(j)
                base_centroid = cluster_centroid([base_centroid, centroids[j]])
        merged.append(base_items)
    return merged

# The write_llms_txt function now returns the content as a string
def write_llms_txt(clusters, results, client):
    lines = []
    lines.append("# llms.txt")
    domain, site_description = generate_site_description(results, client)
    lines.append(f"# {domain}")
    lines.append("")
    lines.append(f"> {site_description}")
    lines.append("")
    for cluster_items in clusters:
        label = cluster_label_from_items(cluster_items, client, fallback="Content")
        lines.append(f"## Cluster: {label}")
        for it in cluster_items:
            lines.append(f"- URL: {it.get('url','')}")
            lines.append(f"  Title: {normalize_space(it.get('title',''))}")
            lines.append(f"  Description: {normalize_space(it.get('description',''))}")
        lines.append("")
    
    # Return the content as a string instead of writing to a file
    return "\n".join(lines)

def build_llms_txt_from_results(results, client, USE_EMBEDDINGS, EMBED_MODEL, SIM_THRESHOLD):
    clusters = seed_clusters(results)
    if USE_EMBEDDINGS and sum(len(c) for c in clusters) > 1:
        clusters = semantically_merge(clusters, client, USE_EMBEDDINGS, EMBED_MODEL, SIM_THRESHOLD)
    for c in clusters: c.sort(key=lambda it: it.get("url",""))
    return write_llms_txt(clusters, results, client=client)
