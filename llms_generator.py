# -*- coding: utf-8 -*-
"""LLMS.TXT Generator

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KCWIg035AuIZyowB_geDF7PxhcazaOsL
"""

# ======================================================================
# All imports are now at the top of the file
# ======================================================================
import streamlit as st
import os
import requests
import xml.etree.ElementTree as ET
import json
from openai import OpenAI
import time
import random
import trafilatura
import re
from urllib.parse import urlparse
from collections import defaultdict, Counter
import math
from datetime import datetime

# ----------------------------------------------------------------------
# Tiny logger helper (prints to console with timestamps)
# ----------------------------------------------------------------------
def log(msg: str):
    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {msg}", flush=True)

# ======================================================================
# All functions are now defined here
# ======================================================================

def _local_name(tag: str) -> str:
    """Return the local name without namespace, e.g. '{ns}url' -> 'url'."""
    return tag.split('}', 1)[-1] if '}' in tag else tag

def extract_urls_from_sitemap(xml_file):
    """
    Parse a sitemap XML and return all <loc> values that are direct
    children of <url> (namespace-agnostic).

    Args:
        xml_file: A file-like object (e.g., from st.file_uploader) or a string path.

    Returns:
        List[str]: URLs found in <url><loc>...</loc></url>
    """
    log("extract_urls_from_sitemap: start")
    # Read bytes from file-like or path
    if hasattr(xml_file, "read"):
        log("extract_urls_from_sitemap: detected file-like object")
        xml_bytes = xml_file.read()
    else:
        log(f"extract_urls_from_sitemap: reading from path: {xml_file}")
        with open(xml_file, "rb") as f:
            xml_bytes = f.read()

    size_kb = round(len(xml_bytes) / 1024, 2)
    log(f"extract_urls_from_sitemap: loaded XML ({size_kb} KB)")

    # Parse XML
    try:
        root = ET.fromstring(xml_bytes)
        log("extract_urls_from_sitemap: XML parsed successfully")
    except ET.ParseError as e:
        log(f"extract_urls_from_sitemap: XML parse error -> {e}")
        raise ValueError(f"Failed to parse XML: {e}") from e

    urls = []
    url_nodes = 0
    # Iterate over all elements; select only <url> nodes by local-name
    for url_el in root.iter():
        if _local_name(url_el.tag) != "url":
            continue
        url_nodes += 1
        # Get direct child <loc> only
        for child in list(url_el):
            if _local_name(child.tag) == "loc" and child.text:
                loc = child.text.strip()
                if loc:
                    urls.append(loc)

    # Optional: de-duplicate while preserving order
    seen = set()
    deduped = []
    for u in urls:
        if u not in seen:
            seen.add(u)
            deduped.append(u)

    log(f"extract_urls_from_sitemap: found {len(urls)} <loc> values across {url_nodes} <url> nodes; {len(deduped)} unique URLs")
    return deduped

def is_relevant_page(url: str, client, EXCLUDE_SEGMENTS, model="gpt-4.1-mini"):
    log(f"is_relevant_page: checking relevance for {url} with model={model}")
    exclude_str = ", ".join(EXCLUDE_SEGMENTS)
    prompt = f"""
You are given a URL.
Assume the page content based on its path and structure.

A page is RELEVANT (content) if it is useful to end users
(e.g. articles, product/service pages, blog posts, guides).

A page is NOT RELEVANT (other) if it looks like a utility or irrelevant page
(e.g. privacy policy, terms, login, settings, cart, thank-you pages,
sitemaps, 404s, or if the URL contains any of these segments:
{exclude_str}).

Return only the word KEEP if the page should be included in the final list.
Return DROP otherwise. When in doubt, just use KEEP.

URL: {url}
"""
    try:
        resp = client.responses.create(
            model=model,
            input=[{"role": "user", "content": prompt}],
            temperature=0,
        )
        txt = resp.output_text.strip().upper()
        keep = txt == "KEEP"
        log(f"is_relevant_page: model returned '{txt}' -> {'KEEP' if keep else 'DROP'}")
        return keep
    except Exception as e:
        log(f"is_relevant_page: ERROR {e} -> defaulting to KEEP")
        return True  # fail-open to KEEP so pipeline doesn't halt

SESSION = requests.Session()
SESSION.headers.update({"User-Agent": "Mozilla/5.0"})

def fetch_text(url: str) -> str:
    log(f"fetch_text: fetching {url}")
    try:
        r = SESSION.get(url, timeout=20)
        r.raise_for_status()
        text = trafilatura.extract(r.text)
        length = len(text) if text else 0
        log(f"fetch_text: success, extracted {length} chars")
        return text or ""
    except Exception as e:
        log(f"fetch_text: ERROR for {url} -> {e}")
        return ""

def describe_page(url: str, page_text: str, client, meta_title="", h1="", model="gpt-4.1-mini"):
    log(f"describe_page: summarizing {url} (ignoring page_text) with model={model}")
    
    base_prompt = f"""You will summarize the page for: {url}

Rules:
- Focus only on the page’s **primary content** (topic, offering, purpose).
- Ignore cookie banners, privacy/legal boilerplate, navigation, and footers.
- Prefer signals from the HTML <title> and H1 if available.

Constraints:
- Title: exactly 3–4 words, English.
- Description: exactly 9–10 words, plain, neutral English.
Return JSON with keys: title, description.

Signals available:
- HTML title: {meta_title}
- H1: {h1}
"""
    try:
        resp = client.responses.create(
            model=model,
            input=[{"role": "user", "content": base_prompt}],
            temperature=0.3,
        )
        txt = resp.output_text.strip()
        if txt.startswith("```"):
            txt = re.sub(r"^```[a-zA-Z]*\n?", "", txt)
            txt = re.sub(r"\n?```$", "", txt)
            txt = txt.strip()
        data = json.loads(txt)
        title = data.get("title", "").strip()
        desc  = data.get("description", "").strip()
        log(f"describe_page: ok -> title='{title}' | desc='{desc}'")
        return {"url": url, "title": title, "description": desc}
    except Exception as e:
        log(f"describe_page: ERROR parsing model output -> {e}")
        return {"url": url, "title": "", "description": ""}

def generate_site_description(results, client, model="gpt-4.1-mini"):
    log(f"generate_site_description: start with {len(results)} results, model={model}")
    if not results: 
        log("generate_site_description: empty results -> fallback description")
        return "General Website", "No description available."
    first_url = results[0]["url"]
    domain = urlparse(first_url).netloc.lower()
    sample_text = "\n".join(
        f"- {r.get('title','') or ''} — {r.get('description','') or ''}"
        for r in results[:30]
    )
    prompt = f"""
You are given a list of page titles and descriptions from the website {domain}.

Write a concise general description (2–3 sentences, neutral English) that summarizes
the main purpose of the site as a whole.

Return only the description text, no extra formatting.

Pages:
{sample_text}
"""
    try:
        resp = client.responses.create(
            model=model,
            input=[{"role": "user", "content": prompt}],
            temperature=0,
        )
        site_description = resp.output_text.strip()
        log(f"generate_site_description: ok for domain={domain}")
        return domain, site_description
    except Exception as e:
        log(f"generate_site_description: ERROR -> {e}")
        return domain, "No description available."

def url_first_segment(u: str) -> str:
    p = urlparse(u)
    segs = [s for s in p.path.split("/") if s]
    return segs[0].lower() if segs else ""

def normalize_space(s: str) -> str:
    return re.sub(r"\s+", " ", s or "").strip()

def dot(a, b): return sum(x*y for x, y in zip(a, b))
def norm(a): return math.sqrt(dot(a, a)) or 1e-12
def cosine(a, b): return dot(a, b) / (norm(a) * norm(b))

def cluster_label_from_items(items, client, fallback="Content", model="gpt-4.1-mini"):
    log(f"cluster_label_from_items: labeling {len(items)} items with model={model}")
    titles = [it.get("title", "").strip() for it in items if it.get("title")]
    if not titles: 
        log("cluster_label_from_items: no titles -> fallback")
        return fallback
    titles_text = "\n".join(f"- {t}" for t in titles if t)
    prompt = f"""
Given the following page titles, suggest a concise English cluster label (2–4 words)
that best represents them as a group.

Return only the label, with no quotes or extra text.

Titles:
{titles_text}
"""
    try:
        resp = client.responses.create(
            model=model,
            input=[{"role": "user", "content": prompt}],
            temperature=0,
        )
        label = resp.output_text.strip()
        label = label if label else fallback
        log(f"cluster_label_from_items: label='{label}'")
        return label
    except Exception as e:
        log(f"cluster_label_from_items: ERROR -> {e} (fallback='{fallback}')")
        return fallback

def seed_clusters(results):
    log(f"seed_clusters: bucketing {len(results)} items by first path segment")
    buckets = defaultdict(list)
    for item in results:
        seg = url_first_segment(item.get("url","")) or "root"
        buckets[seg].append(item)
    sizes = {k: len(v) for k, v in buckets.items()}
    log(f"seed_clusters: {len(buckets)} buckets -> sizes {sizes}")
    return [items for _, items in sorted(buckets.items(), key=lambda kv: kv[0])]

def embed_texts(texts, client, EMBED_MODEL):
    log(f"embed_texts: embedding {len(texts)} texts with model={EMBED_MODEL}")
    resp = client.embeddings.create(model=EMBED_MODEL, input=texts)
    log("embed_texts: embeddings created")
    return [d.embedding for d in resp.data]

def represent_item(item):
    return normalize_space(f'{item.get("title","")} — {item.get("description","")}')

def cluster_centroid(vecs):
    if not vecs: return []
    dim = len(vecs[0])
    s = [0.0]*dim
    for v in vecs:
        for i in range(dim): s[i] += v[i]
    return [x/len(vecs) for x in s]

def semantically_merge(clusters, client, USE_EMBEDDINGS, EMBED_MODEL, SIM_THRESHOLD):
    log(f"semantically_merge: start with {len(clusters)} clusters | threshold={SIM_THRESHOLD} | model={EMBED_MODEL}")
    texts = []
    idx_map = []
    for ci, items in enumerate(clusters):
        for ii, it in enumerate(items):
            texts.append(represent_item(it))
            idx_map.append((ci, ii))
    if not texts:
        log("semantically_merge: no texts -> return original clusters")
        return clusters

    vecs = embed_texts(texts, client, EMBED_MODEL)
    per_cluster_vecs = defaultdict(list)
    for (ci, _), v in zip(idx_map, vecs):
        per_cluster_vecs[ci].append(v)
    centroids = [cluster_centroid(per_cluster_vecs[i]) for i in range(len(clusters))]

    merged = []
    used = set()
    merge_ops = []
    for i in range(len(clusters)):
        if i in used: continue
        base_items = list(clusters[i])
        base_centroid = centroids[i]
        used.add(i)
        for j in range(i+1, len(clusters)):
            if j in used or not base_centroid or not centroids[j]: continue
            sim = cosine(base_centroid, centroids[j])
            if sim >= SIM_THRESHOLD:
                merge_ops.append((i, j, round(sim, 4)))
                base_items.extend(clusters[j])
                used.add(j)
                base_centroid = cluster_centroid([base_centroid, centroids[j]])
        merged.append(base_items)

    if merge_ops:
        log(f"semantically_merge: merged pairs {merge_ops}")
    else:
        log("semantically_merge: no merges performed")

    log(f"semantically_merge: end -> {len(merged)} clusters")
    return merged

def write_llms_txt(clusters, results, client):
    log(f"write_llms_txt: rendering {sum(len(c) for c in clusters)} items across {len(clusters)} clusters")
    lines = []
    domain, site_description = generate_site_description(results, client)

    # First line = "# Domain" (capitalized first letter)
    domain_line = f"# {domain.strip().capitalize()}"
    lines.append(domain_line)
    log(f"write_llms_txt: header -> {domain_line}")

    # Optional site description as a quote (keep if present)
    if site_description and str(site_description).strip():
        lines.append("")
        lines.append(f"> {site_description.strip()}")
        lines.append("")
        log("write_llms_txt: added site description")

    # Clusters
    for idx, cluster_items in enumerate(clusters, start=1):
        label = cluster_label_from_items(cluster_items, client, fallback="Content")
        lines.append(f"## Cluster: {label}")
        log(f"write_llms_txt: cluster {idx} label='{label}' size={len(cluster_items)}")

        for it in cluster_items:
            url = (it.get("url", "") or "").strip()
            title = normalize_space(it.get("title", "") or "").strip()
            desc = normalize_space(it.get("description", "") or "").strip()

            if not title:
                title = url

            bullet = f"- [{title}]({url})"
            if desc:
                bullet += f": {desc}"
            lines.append(bullet)

        lines.append("")

    output = "\n".join(lines)
    log(f"write_llms_txt: done (length={len(output)} chars)")
    return output

def build_llms_txt_from_results(results, client, USE_EMBEDDINGS, EMBED_MODEL, SIM_THRESHOLD):
    log(f"build_llms_txt_from_results: start | results={len(results)} | use_embeddings={USE_EMBEDDINGS} | model={EMBED_MODEL} | thr={SIM_THRESHOLD}")
    clusters = seed_clusters(results)
    total_items = sum(len(c) for c in clusters)
    log(f"build_llms_txt_from_results: seeded {len(clusters)} clusters with {total_items} items total")
    if USE_EMBEDDINGS and total_items > 1:
        clusters = semantically_merge(clusters, client, USE_EMBEDDINGS, EMBED_MODEL, SIM_THRESHOLD)
    for c in clusters:
        c.sort(key=lambda it: it.get("url", ""))
    log("build_llms_txt_from_results: clusters sorted by URL")
    final_txt = write_llms_txt(clusters, results, client=client)
    log("build_llms_txt_from_results: completed llms.txt build")
    return final_txt
